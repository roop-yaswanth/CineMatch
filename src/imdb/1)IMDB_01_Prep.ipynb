{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "hSFUCY6lCzBR",
      "metadata": {
        "id": "hSFUCY6lCzBR"
      },
      "source": [
        "# IMDb 01 Prep (Movie-Only Catalog)\n",
        "\n",
        "Objective:\n",
        "- Build a movie-only IMDb catalog with TMDB plot enrichment.\n",
        "- Aggregate top-3 AKA titles and infer multilingual language buckets.\n",
        "- Produce `imdb_movies_catalog.csv`, `imdb_movies_meta.csv`, and prep stats in manifest.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dXNvYW9SC39B",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXNvYW9SC39B",
        "outputId": "b01aae8c-5017-47cd-c79b-ffb2b86ff751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "073a1ca7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "073a1ca7",
        "outputId": "d34c245f-d7b2-4f16-8fe4-c4d51cdf046b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'/content/drive/MyDrive/cinematch/Data/IMDB0226.zip' extracted to '/content/drive/MyDrive/cinematch/Data/IMDB' successfully.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/cinematch/Data/IMDB0226.zip'\n",
        "extraction_path = '/content/drive/MyDrive/cinematch/Data/IMDB'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "\n",
        "# Open the zip file in read mode\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all the contents to the specified directory\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(f\"'{zip_file_path}' extracted to '{extraction_path}' successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e03f55bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e03f55bb",
        "outputId": "2a987b77-2477-406f-af24-46e8c1aa6cd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 7 .tsv.gz files. Extracting...\n",
            "Extracting name.basics.tsv.gz to name.basics.tsv...\n",
            "Successfully extracted name.basics.tsv.gz\n",
            "Extracting title.akas.tsv.gz to title.akas.tsv...\n",
            "Successfully extracted title.akas.tsv.gz\n",
            "Extracting title.basics.tsv.gz to title.basics.tsv...\n",
            "Successfully extracted title.basics.tsv.gz\n",
            "Extracting title.crew.tsv.gz to title.crew.tsv...\n",
            "Successfully extracted title.crew.tsv.gz\n",
            "Extracting title.episode.tsv.gz to title.episode.tsv...\n",
            "Successfully extracted title.episode.tsv.gz\n",
            "Extracting title.principals.tsv.gz to title.principals.tsv...\n",
            "Successfully extracted title.principals.tsv.gz\n",
            "Extracting title.ratings.tsv.gz to title.ratings.tsv...\n",
            "Successfully extracted title.ratings.tsv.gz\n",
            "All .tsv.gz file extraction process completed.\n"
          ]
        }
      ],
      "source": [
        "import gzip\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "imdb_data_dir = Path('/content/drive/MyDrive/cinematch/Data/IMDB')\n",
        "\n",
        "if not imdb_data_dir.is_dir():\n",
        "    print(f\"Error: Directory not found at {imdb_data_dir}\")\n",
        "else:\n",
        "    tsv_gz_files = list(imdb_data_dir.glob('*.tsv.gz'))\n",
        "\n",
        "    if not tsv_gz_files:\n",
        "        print(f\"No .tsv.gz files found in {imdb_data_dir}\")\n",
        "    else:\n",
        "        print(f\"Found {len(tsv_gz_files)} .tsv.gz files. Extracting...\")\n",
        "        for gz_file_path in tsv_gz_files:\n",
        "            tsv_file_path = gz_file_path.with_suffix('') # Remove .gz suffix\n",
        "            print(f\"Extracting {gz_file_path.name} to {tsv_file_path.name}...\")\n",
        "            try:\n",
        "                with gzip.open(gz_file_path, 'rb') as f_in:\n",
        "                    with open(tsv_file_path, 'wb') as f_out:\n",
        "                        shutil.copyfileobj(f_in, f_out)\n",
        "                print(f\"Successfully extracted {gz_file_path.name}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting {gz_file_path.name}: {e}\")\n",
        "        print(\"All .tsv.gz file extraction process completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1wQdABuxCzBS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wQdABuxCzBS",
        "outputId": "79cc5ce3-7b4e-4ec2-ca28-76a8e111e60d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMDb directory: /content/drive/MyDrive/cinematch/Data/IMDB\n",
            "TMDB path: /content/drive/MyDrive/cinematch/Data/TMDB_movie_dataset_v11.csv\n",
            "Output directory: /content/drive/MyDrive/cinematch/outputs/imdb\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Set\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "\n",
        "IMDB_DIR = Path('/content/drive/MyDrive/cinematch/Data/IMDB')\n",
        "TMDB_PATH = Path('/content/drive/MyDrive/cinematch/Data/TMDB_movie_dataset_v11.csv')\n",
        "OUT_DIR = Path('/content/drive/MyDrive/cinematch/outputs/imdb')\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "BASICS_PATH = IMDB_DIR / 'title.basics.tsv'\n",
        "RATINGS_PATH = IMDB_DIR / 'title.ratings.tsv'\n",
        "AKAS_PATH = IMDB_DIR / 'title.akas.tsv'\n",
        "\n",
        "CATALOG_PATH = OUT_DIR / 'imdb_movies_catalog.csv'\n",
        "META_PATH = OUT_DIR / 'imdb_movies_meta.csv'\n",
        "MANIFEST_PATH = OUT_DIR / 'imdb_movies_build_manifest.json'\n",
        "\n",
        "CHUNKSIZE_BASICS = 1_000_000\n",
        "CHUNKSIZE_AKAS = 1_000_000\n",
        "TOP_AKAS = 3\n",
        "\n",
        "TARGET_LANGS = ['te', 'hi', 'ta', 'ja', 'ko']\n",
        "TARGET_LANG_SET = set(TARGET_LANGS)\n",
        "\n",
        "print('IMDb directory:', IMDB_DIR)\n",
        "print('TMDB path:', TMDB_PATH)\n",
        "print('Output directory:', OUT_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pnrQFx9tCzBT",
      "metadata": {
        "id": "pnrQFx9tCzBT"
      },
      "source": [
        "## Utility Functions\n",
        "\n",
        "These helpers implement ranking for AKA titles, script detection fallback, language bucket inference, and `movieDoc` creation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bNt7WPeUCzBT",
      "metadata": {
        "id": "bNt7WPeUCzBT"
      },
      "outputs": [],
      "source": [
        "def clean_value(v: str) -> str:\n",
        "    if v is None:\n",
        "        return ''\n",
        "    s = str(v).strip()\n",
        "    return '' if s in {'', '\\\\N', 'nan', 'NaN', '<NA>'} else s\n",
        "\n",
        "\n",
        "def clean_int(v: str):\n",
        "    s = clean_value(v)\n",
        "    if not s:\n",
        "        return pd.NA\n",
        "    try:\n",
        "        return int(float(s))\n",
        "    except Exception:\n",
        "        return pd.NA\n",
        "\n",
        "\n",
        "def clean_float(v: str):\n",
        "    s = clean_value(v)\n",
        "    if not s:\n",
        "        return np.nan\n",
        "    try:\n",
        "        return float(s)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def contains_imdb_display(types_value: str) -> int:\n",
        "    parts = [p.strip() for p in clean_value(types_value).split(',') if p.strip()]\n",
        "    return 1 if 'imdbDisplay' in parts else 0\n",
        "\n",
        "\n",
        "def rank_tuple(is_original: str, types_value: str, ordering: str, title: str):\n",
        "    is_orig = 1 if clean_value(is_original) == '1' else 0\n",
        "    has_imdb_display = contains_imdb_display(types_value)\n",
        "    ord_val = clean_int(ordering)\n",
        "    ord_num = int(ord_val) if ord_val is not pd.NA else 10**9\n",
        "    return (-is_orig, -has_imdb_display, ord_num, title.casefold())\n",
        "\n",
        "\n",
        "def update_top_k_candidates(candidates: List[dict], new_candidate: dict, k: int = 3) -> None:\n",
        "    replaced = False\n",
        "    for i, old in enumerate(candidates):\n",
        "        if old['title'] == new_candidate['title']:\n",
        "            if new_candidate['rank'] < old['rank']:\n",
        "                candidates[i] = new_candidate\n",
        "            else:\n",
        "                if not old.get('language') and new_candidate.get('language'):\n",
        "                    old['language'] = new_candidate['language']\n",
        "                if not old.get('region') and new_candidate.get('region'):\n",
        "                    old['region'] = new_candidate['region']\n",
        "            replaced = True\n",
        "            break\n",
        "\n",
        "    if not replaced:\n",
        "        candidates.append(new_candidate)\n",
        "\n",
        "    candidates.sort(key=lambda x: x['rank'])\n",
        "    if len(candidates) > k:\n",
        "        del candidates[k:]\n",
        "\n",
        "\n",
        "SCRIPT_RANGES = {\n",
        "    'te': [(0x0C00, 0x0C7F)],\n",
        "    'ta': [(0x0B80, 0x0BFF)],\n",
        "    'hi': [(0x0900, 0x097F)],\n",
        "    'ko': [(0x1100, 0x11FF), (0x3130, 0x318F), (0xAC00, 0xD7AF)],\n",
        "    'ja': [(0x3040, 0x309F), (0x30A0, 0x30FF), (0x4E00, 0x9FFF)],\n",
        "}\n",
        "\n",
        "\n",
        "def has_script(text: str, ranges: list[tuple[int, int]]) -> bool:\n",
        "    for ch in text:\n",
        "        code = ord(ch)\n",
        "        for lo, hi in ranges:\n",
        "            if lo <= code <= hi:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def detect_script_bucket(text: str) -> str:\n",
        "    t = clean_value(text)\n",
        "    if not t:\n",
        "        return ''\n",
        "    for lang in ['te', 'ta', 'hi', 'ko', 'ja']:\n",
        "        if has_script(t, SCRIPT_RANGES[lang]):\n",
        "            return lang\n",
        "    return ''\n",
        "\n",
        "\n",
        "\n",
        "def infer_lang_bucket(\n",
        "    aka_langs: Set[str],\n",
        "    aka_regions: Set[str],\n",
        "    akas_top3: str,\n",
        "    primary_title: str,\n",
        "    original_title: str,\n",
        "    tmdb_original_language: str = '',\n",
        ") -> str:\n",
        "    # TMDB original_language â€” most reliable signal\n",
        "    tmdb_lang = clean_value(tmdb_original_language)\n",
        "    if tmdb_lang in TARGET_LANG_SET:\n",
        "        return tmdb_lang\n",
        "\n",
        "    # AKA language codes\n",
        "    for lang in ['te', 'ta', 'hi', 'ja', 'ko']:\n",
        "        if lang in aka_langs:\n",
        "            return lang\n",
        "\n",
        "    script_source = ' '.join([clean_value(akas_top3), clean_value(primary_title), clean_value(original_title)])\n",
        "    script_bucket = detect_script_bucket(script_source)\n",
        "    if script_bucket:\n",
        "        return script_bucket\n",
        "\n",
        "    if 'JP' in aka_regions:\n",
        "        return 'ja'\n",
        "    if 'KR' in aka_regions:\n",
        "        return 'ko'\n",
        "    if 'IN' in aka_regions:\n",
        "        return 'other_non_en'\n",
        "\n",
        "    if tmdb_lang and tmdb_lang != 'en':\n",
        "        return 'other_non_en'\n",
        "\n",
        "    if 'en' in aka_langs:\n",
        "        return 'en'\n",
        "    non_empty_langs = {l for l in aka_langs if clean_value(l)}\n",
        "    if non_empty_langs:\n",
        "        return 'other_non_en'\n",
        "\n",
        "    if clean_value(primary_title) or clean_value(original_title):\n",
        "        merged = clean_value(primary_title) + clean_value(original_title)\n",
        "        try:\n",
        "            merged.encode('ascii')\n",
        "            return 'en'\n",
        "        except UnicodeEncodeError:\n",
        "            return 'unknown'\n",
        "\n",
        "    return 'unknown'\n",
        "\n",
        "\n",
        "def infer_origin_lang_bucket(\n",
        "    original_title_langs: Set[str],\n",
        "    aka_regions: Set[str],\n",
        "    primary_title: str,\n",
        "    original_title: str,\n",
        "    tmdb_original_language: str = '',\n",
        ") -> str:\n",
        "    tmdb_lang = clean_value(tmdb_original_language)\n",
        "    if tmdb_lang in TARGET_LANG_SET:\n",
        "        return tmdb_lang\n",
        "    for lang in ['te', 'ta', 'hi', 'ja', 'ko', 'en']:\n",
        "        if lang in original_title_langs:\n",
        "            return lang\n",
        "\n",
        "    non_empty_original = {l for l in original_title_langs if clean_value(l)}\n",
        "    if non_empty_original:\n",
        "        return 'other_non_en'\n",
        "    if tmdb_lang and tmdb_lang != 'en':\n",
        "        return 'other_non_en'\n",
        "\n",
        "    script_source = ' '.join([clean_value(original_title), clean_value(primary_title)])\n",
        "    script_bucket = detect_script_bucket(script_source)\n",
        "    if script_bucket:\n",
        "        return script_bucket\n",
        "\n",
        "    if 'JP' in aka_regions:\n",
        "        return 'ja'\n",
        "    if 'KR' in aka_regions:\n",
        "        return 'ko'\n",
        "    if 'IN' in aka_regions:\n",
        "        return 'other_non_en'\n",
        "\n",
        "    if clean_value(primary_title) or clean_value(original_title):\n",
        "        merged = clean_value(primary_title) + clean_value(original_title)\n",
        "        try:\n",
        "            merged.encode('ascii')\n",
        "            return 'en'\n",
        "        except UnicodeEncodeError:\n",
        "            return 'unknown'\n",
        "\n",
        "    return 'unknown'\n",
        "\n",
        "\n",
        "def movie_doc_from_row(row: pd.Series) -> str:\n",
        "    title = clean_value(row.get('primaryTitle'))\n",
        "    original = clean_value(row.get('originalTitle'))\n",
        "    akas_top3 = clean_value(row.get('akas_top3'))\n",
        "    year = clean_value(row.get('startYear'))\n",
        "    runtime = clean_value(row.get('runtimeMinutes'))\n",
        "    genres = clean_value(row.get('genres'))\n",
        "    rating = row.get('averageRating')\n",
        "    votes = row.get('numVotes')\n",
        "    overview = clean_value(row.get('tmdb_overview'))\n",
        "\n",
        "    parts = []\n",
        "    parts.append(f'Title: {title if title else \"Unknown\"}')\n",
        "    if original and original != title:\n",
        "        parts.append(f'Original Title: {original}')\n",
        "    if akas_top3:\n",
        "        parts.append(f'AKA: {akas_top3}')\n",
        "    if year:\n",
        "        parts.append(f'Year: {year}')\n",
        "    if runtime:\n",
        "        parts.append(f'Runtime Minutes: {runtime}')\n",
        "    if genres:\n",
        "        parts.append(f'Genres: {genres}')\n",
        "\n",
        "    rating_str = '' if pd.isna(rating) else f'{float(rating):.1f}'\n",
        "    votes_str = '' if pd.isna(votes) else str(int(votes))\n",
        "    if rating_str or votes_str:\n",
        "        parts.append(f'IMDb Rating: {rating_str if rating_str else \"NA\"} (Votes: {votes_str if votes_str else \"NA\"})')\n",
        "\n",
        "    if overview:\n",
        "        parts.append(f'Plot: {overview}')\n",
        "    else:\n",
        "        fallback_plot = (\n",
        "            'Plot: Metadata-only profile. '\n",
        "            f'No external summary found. Genres: {genres if genres else \"unknown\"}; Year: {year if year else \"unknown\"}.'\n",
        "        )\n",
        "        parts.append(fallback_plot)\n",
        "\n",
        "    return '\\n'.join(parts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yZgljpOGCzBU",
      "metadata": {
        "id": "yZgljpOGCzBU"
      },
      "source": [
        "## Load Movie Basics + Ratings + TMDB Enrichment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5OmVwtl2CzBU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OmVwtl2CzBU",
        "outputId": "7cfee245-a90e-47c3-861f-efba6668eaa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Movies loaded: 737654\n",
            "Movies with TMDB overview: 301717\n",
            "TMDB overview coverage: 0.409\n",
            "Movies with TMDB original_language: 351167\n"
          ]
        }
      ],
      "source": [
        "prep_started = time.time()\n",
        "\n",
        "basic_cols = [\n",
        "    'tconst',\n",
        "    'titleType',\n",
        "    'primaryTitle',\n",
        "    'originalTitle',\n",
        "    'isAdult',\n",
        "    'startYear',\n",
        "    'runtimeMinutes',\n",
        "    'genres',\n",
        "]\n",
        "\n",
        "movie_parts = []\n",
        "for chunk in pd.read_csv(\n",
        "    BASICS_PATH,\n",
        "    sep='\\t',\n",
        "    dtype=str,\n",
        "    usecols=basic_cols,\n",
        "    chunksize=CHUNKSIZE_BASICS,\n",
        "    na_filter=False,\n",
        "):\n",
        "    movie_chunk = chunk.loc[chunk['titleType'] == 'movie', basic_cols].copy()\n",
        "    if not movie_chunk.empty:\n",
        "        movie_parts.append(movie_chunk)\n",
        "\n",
        "movies = pd.concat(movie_parts, ignore_index=True)\n",
        "movies = movies.sort_values('tconst').reset_index(drop=True)\n",
        "\n",
        "for col in ['primaryTitle', 'originalTitle', 'genres']:\n",
        "    movies[col] = movies[col].map(clean_value)\n",
        "\n",
        "movies['isAdult'] = movies['isAdult'].map(clean_int).fillna(0).astype('int64')\n",
        "movies['startYear'] = movies['startYear'].map(clean_int).astype('Int64')\n",
        "movies['runtimeMinutes'] = movies['runtimeMinutes'].map(clean_int).astype('Int64')\n",
        "\n",
        "ratings = pd.read_csv(RATINGS_PATH, sep='\\t', dtype=str, na_filter=False)\n",
        "ratings['averageRating'] = ratings['averageRating'].map(clean_float)\n",
        "ratings['numVotes'] = ratings['numVotes'].map(clean_int).astype('Int64')\n",
        "ratings = ratings[['tconst', 'averageRating', 'numVotes']]\n",
        "\n",
        "movies = movies.merge(ratings, on='tconst', how='left')\n",
        "\n",
        "\n",
        "tmdb_cols = ['id', 'imdb_id', 'title', 'overview', 'original_language',\n",
        "             'vote_count', 'popularity', 'release_date']\n",
        "tmdb = pd.read_csv(TMDB_PATH, usecols=tmdb_cols, low_memory=False)\n",
        "tmdb['imdb_id'] = tmdb['imdb_id'].fillna('').astype(str).str.strip()\n",
        "tmdb = tmdb[tmdb['imdb_id'].str.match(r'^tt\\d+$', na=False)].copy()\n",
        "tmdb['overview'] = tmdb['overview'].fillna('').astype(str).str.strip()\n",
        "tmdb['has_overview'] = (tmdb['overview'].str.len() >= 20).astype(int)\n",
        "tmdb['vote_count'] = pd.to_numeric(tmdb['vote_count'], errors='coerce').fillna(0)\n",
        "tmdb['popularity'] = pd.to_numeric(tmdb['popularity'], errors='coerce').fillna(0)\n",
        "tmdb['release_date'] = tmdb['release_date'].fillna('').astype(str)\n",
        "tmdb['id'] = pd.to_numeric(tmdb['id'], errors='coerce').fillna(0)\n",
        "tmdb['original_language'] = tmdb['original_language'].fillna('').astype(str).str.strip()\n",
        "\n",
        "tmdb = tmdb.sort_values(\n",
        "    ['has_overview', 'vote_count', 'popularity', 'release_date', 'id'],\n",
        "    ascending=[False, False, False, False, False],\n",
        ")\n",
        "tmdb_best = tmdb.drop_duplicates(subset=['imdb_id'], keep='first').copy()\n",
        "tmdb_best = tmdb_best.rename(\n",
        "    columns={\n",
        "        'imdb_id': 'tconst',\n",
        "        'id': 'tmdb_id',\n",
        "        'title': 'tmdb_title',\n",
        "        'overview': 'tmdb_overview',\n",
        "        'original_language': 'tmdb_original_language',   # NEW\n",
        "    }\n",
        ")\n",
        "tmdb_best = tmdb_best[['tconst', 'tmdb_id', 'tmdb_title', 'tmdb_overview', 'tmdb_original_language']]\n",
        "\n",
        "movies = movies.merge(tmdb_best, on='tconst', how='left')\n",
        "movies['tmdb_overview'] = movies['tmdb_overview'].fillna('').astype(str).str.strip()\n",
        "movies['tmdb_original_language'] = movies['tmdb_original_language'].fillna('').astype(str).str.strip()\n",
        "movies['has_tmdb_plot'] = (movies['tmdb_overview'].str.len() >= 20)\n",
        "\n",
        "print('Movies loaded:', len(movies))\n",
        "print('Movies with TMDB overview:', int(movies['has_tmdb_plot'].sum()))\n",
        "print('TMDB overview coverage:', round(float(movies['has_tmdb_plot'].mean()), 4))\n",
        "print('Movies with TMDB original_language:', int((movies['tmdb_original_language'] != '').sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gfqv5W3KCzBU",
      "metadata": {
        "id": "gfqv5W3KCzBU"
      },
      "source": [
        "## Process AKA Titles (Top-3) + Infer Language Bucket + Build `movieDoc`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6mj9lERFCzBU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mj9lERFCzBU",
        "outputId": "f97dc6f4-592a-4194-8837-49d4fa3d697f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AKA rows seen: 54958521\n",
            "AKA rows matched to movies: 3674721\n",
            "Movies with at least one AKA row: 736117\n",
            "Saved catalog: /content/drive/MyDrive/cinematch/outputs/imdb/imdb_movies_catalog.csv\n",
            "Saved meta: /content/drive/MyDrive/cinematch/outputs/imdb/imdb_movies_meta.csv\n",
            "Updated manifest: /content/drive/MyDrive/cinematch/outputs/imdb/imdb_movies_build_manifest.json\n",
            "Top availability language buckets: {'en': 393262, 'other_non_en': 191827, 'ja': 92256, 'hi': 24205, 'unknown': 20712, 'ko': 8944, 'ta': 3709, 'te': 2739}\n",
            "Top origin language buckets: {'en': 405045, 'other_non_en': 210600, 'ja': 76366, 'unknown': 24527, 'ko': 8825, 'hi': 6495, 'ta': 3386, 'te': 2410}\n"
          ]
        }
      ],
      "source": [
        "movie_set = set(movies['tconst'].tolist())\n",
        "\n",
        "aka_top_map: Dict[str, List[dict]] = defaultdict(list)\n",
        "aka_langs_map: Dict[str, Set[str]] = defaultdict(set)\n",
        "aka_regions_map: Dict[str, Set[str]] = defaultdict(set)\n",
        "aka_original_langs_map: Dict[str, Set[str]] = defaultdict(set)\n",
        "\n",
        "akas_cols = ['titleId', 'ordering', 'title', 'region', 'language', 'types', 'isOriginalTitle']\n",
        "rows_seen = 0\n",
        "rows_matched_movies = 0\n",
        "\n",
        "for chunk in pd.read_csv(\n",
        "    AKAS_PATH,\n",
        "    sep='\t',\n",
        "    dtype=str,\n",
        "    usecols=akas_cols,\n",
        "    chunksize=CHUNKSIZE_AKAS,\n",
        "    na_filter=False,\n",
        "):\n",
        "    rows_seen += len(chunk)\n",
        "    chunk = chunk[chunk['titleId'].isin(movie_set)]\n",
        "    rows_matched_movies += len(chunk)\n",
        "    if chunk.empty:\n",
        "        continue\n",
        "\n",
        "    for row in chunk.itertuples(index=False):\n",
        "        tconst = clean_value(row.titleId)\n",
        "        title = clean_value(row.title)\n",
        "        if not tconst or not title:\n",
        "            continue\n",
        "\n",
        "        language = clean_value(row.language)\n",
        "        region = clean_value(row.region)\n",
        "        is_original = clean_value(row.isOriginalTitle) == '1'\n",
        "\n",
        "        if language:\n",
        "            aka_langs_map[tconst].add(language)\n",
        "            if is_original:\n",
        "                aka_original_langs_map[tconst].add(language)\n",
        "        if region:\n",
        "            aka_regions_map[tconst].add(region)\n",
        "\n",
        "        candidate = {\n",
        "            'rank': rank_tuple(row.isOriginalTitle, row.types, row.ordering, title),\n",
        "            'title': title,\n",
        "            'language': language,\n",
        "            'region': region,\n",
        "        }\n",
        "        update_top_k_candidates(aka_top_map[tconst], candidate, k=TOP_AKAS)\n",
        "\n",
        "print('AKA rows seen:', rows_seen)\n",
        "print('AKA rows matched to movies:', rows_matched_movies)\n",
        "print('Movies with at least one AKA row:', len(aka_top_map))\n",
        "\n",
        "akas_top3_map = {}\n",
        "akas_count_map = {}\n",
        "for tconst, candidates in aka_top_map.items():\n",
        "    titles = []\n",
        "    seen_titles = set()\n",
        "    for item in sorted(candidates, key=lambda x: x['rank']):\n",
        "        if item['title'] not in seen_titles:\n",
        "            titles.append(item['title'])\n",
        "            seen_titles.add(item['title'])\n",
        "    akas_top3_map[tconst] = ' | '.join(titles)\n",
        "    akas_count_map[tconst] = len(titles)\n",
        "\n",
        "movies['akas_top3'] = movies['tconst'].map(akas_top3_map).fillna('')\n",
        "movies['akas_top3_count'] = movies['tconst'].map(akas_count_map).fillna(0).astype('int64')\n",
        "movies['aka_langs'] = movies['tconst'].map(aka_langs_map).apply(lambda x: x if isinstance(x, set) else set())\n",
        "movies['aka_regions'] = movies['tconst'].map(aka_regions_map).apply(lambda x: x if isinstance(x, set) else set())\n",
        "movies['aka_original_langs'] = movies['tconst'].map(aka_original_langs_map).apply(lambda x: x if isinstance(x, set) else set())\n",
        "\n",
        "movies['lang_bucket'] = movies.apply(\n",
        "    lambda r: infer_lang_bucket(\n",
        "        aka_langs=r['aka_langs'],\n",
        "        aka_regions=r['aka_regions'],\n",
        "        akas_top3=r['akas_top3'],\n",
        "        primary_title=r['primaryTitle'],\n",
        "        original_title=r['originalTitle'],\n",
        "        tmdb_original_language=r.get('tmdb_original_language', ''),\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "movies['origin_lang_bucket'] = movies.apply(\n",
        "    lambda r: infer_origin_lang_bucket(\n",
        "        original_title_langs=r['aka_original_langs'],\n",
        "        aka_regions=r['aka_regions'],\n",
        "        primary_title=r['primaryTitle'],\n",
        "        original_title=r['originalTitle'],\n",
        "        tmdb_original_language=r.get('tmdb_original_language', ''),\n",
        "    ),\n",
        "    axis=1,\n",
        ")\n",
        "\n",
        "movies['movieDoc'] = movies.apply(movie_doc_from_row, axis=1)\n",
        "\n",
        "movies = movies.sort_values('tconst').reset_index(drop=True)\n",
        "movies['row_id'] = movies.index.astype('int64')\n",
        "\n",
        "movies['aka_langs_codes'] = movies['aka_langs'].apply(lambda s: ','.join(sorted(s)) if s else '')\n",
        "movies['aka_regions_codes'] = movies['aka_regions'].apply(lambda s: ','.join(sorted(s)) if s else '')\n",
        "movies['aka_original_langs_codes'] = movies['aka_original_langs'].apply(lambda s: ','.join(sorted(s)) if s else '')\n",
        "\n",
        "catalog_cols = [\n",
        "    'row_id',\n",
        "    'tconst',\n",
        "    'titleType',\n",
        "    'primaryTitle',\n",
        "    'originalTitle',\n",
        "    'startYear',\n",
        "    'runtimeMinutes',\n",
        "    'genres',\n",
        "    'isAdult',\n",
        "    'averageRating',\n",
        "    'numVotes',\n",
        "    'tmdb_overview',\n",
        "    'has_tmdb_plot',\n",
        "    'akas_top3',\n",
        "    'lang_bucket',\n",
        "    'origin_lang_bucket',\n",
        "    'tmdb_original_language',\n",
        "    'movieDoc',\n",
        "]\n",
        "\n",
        "meta_cols = [\n",
        "    'row_id',\n",
        "    'tconst',\n",
        "    'primaryTitle',\n",
        "    'startYear',\n",
        "    'genres',\n",
        "    'averageRating',\n",
        "    'numVotes',\n",
        "    'lang_bucket',\n",
        "    'origin_lang_bucket',\n",
        "    'tmdb_original_language',\n",
        "    'has_tmdb_plot',\n",
        "]\n",
        "\n",
        "catalog = movies[catalog_cols].copy()\n",
        "meta = movies[meta_cols].copy()\n",
        "\n",
        "catalog.to_csv(CATALOG_PATH, index=False)\n",
        "meta.to_csv(META_PATH, index=False)\n",
        "\n",
        "lang_counts = Counter(catalog['lang_bucket'])\n",
        "origin_lang_counts = Counter(catalog['origin_lang_bucket'])\n",
        "prep_seconds = time.time() - prep_started\n",
        "\n",
        "manifest = {}\n",
        "if MANIFEST_PATH.exists():\n",
        "    manifest = json.loads(MANIFEST_PATH.read_text(encoding='utf-8'))\n",
        "\n",
        "manifest['prep'] = {\n",
        "    'timestamp_utc': pd.Timestamp.utcnow().isoformat(),\n",
        "    'movie_rows': int(len(catalog)),\n",
        "    'tmdb_plot_rows': int(catalog['has_tmdb_plot'].sum()),\n",
        "    'tmdb_plot_coverage': float(catalog['has_tmdb_plot'].mean()),\n",
        "    'lang_bucket_counts': {k: int(v) for k, v in lang_counts.items()},\n",
        "    'origin_lang_bucket_counts': {k: int(v) for k, v in origin_lang_counts.items()},\n",
        "    'debias_bucket_column': 'origin_lang_bucket',\n",
        "    'inputs': {\n",
        "        'title_basics': str(BASICS_PATH),\n",
        "        'title_ratings': str(RATINGS_PATH),\n",
        "        'title_akas': str(AKAS_PATH),\n",
        "        'tmdb_csv': str(TMDB_PATH),\n",
        "    },\n",
        "    'outputs': {\n",
        "        'catalog_csv': str(CATALOG_PATH),\n",
        "        'meta_csv': str(META_PATH),\n",
        "    },\n",
        "    'chunk_sizes': {\n",
        "        'basics': CHUNKSIZE_BASICS,\n",
        "        'akas': CHUNKSIZE_AKAS,\n",
        "    },\n",
        "    'duration_seconds': round(prep_seconds, 2),\n",
        "}\n",
        "\n",
        "MANIFEST_PATH.write_text(json.dumps(manifest, indent=2), encoding='utf-8')\n",
        "\n",
        "print('Saved catalog:', CATALOG_PATH)\n",
        "print('Saved meta:', META_PATH)\n",
        "print('Updated manifest:', MANIFEST_PATH)\n",
        "print('Top availability language buckets:', dict(lang_counts.most_common(10)))\n",
        "print('Top origin language buckets:', dict(origin_lang_counts.most_common(10)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JHR8PA4mCzBV",
      "metadata": {
        "id": "JHR8PA4mCzBV"
      },
      "source": [
        "## Acceptance Checks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nVq4nFHdCzBV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVq4nFHdCzBV",
        "outputId": "69155f1b-eea0-4283-8d24-107c4d438e85"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'catalog_rows': 737654,\n",
              " 'meta_rows': 737654,\n",
              " 'tmdb_plot_rows': 301717,\n",
              " 'max_akas_per_movie': 3}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assert len(catalog) == len(movies), 'Catalog row count mismatch.'\n",
        "\n",
        "assert catalog['row_id'].is_unique, 'row_id must be unique.'\n",
        "assert int(catalog['row_id'].min()) == 0, 'row_id must start at 0.'\n",
        "assert int(catalog['row_id'].max()) == len(catalog) - 1, 'row_id must be contiguous.'\n",
        "\n",
        "assert not catalog['tconst'].duplicated().any(), 'Duplicate tconst found in catalog.'\n",
        "\n",
        "max_akas = int(movies['akas_top3_count'].max())\n",
        "assert max_akas <= TOP_AKAS, 'More than TOP_AKAS found in akas_top3.'\n",
        "\n",
        "summary = {\n",
        "    'catalog_rows': int(len(catalog)),\n",
        "    'meta_rows': int(len(meta)),\n",
        "    'tmdb_plot_rows': int(catalog['has_tmdb_plot'].sum()),\n",
        "    'max_akas_per_movie': int(max_akas),\n",
        "}\n",
        "summary\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
